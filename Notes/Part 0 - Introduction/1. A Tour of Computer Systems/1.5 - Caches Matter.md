### Core Idea
- Most program execution time involves moving data between different storage locations, not performing computation.
- Since larger storage is slower and smaller storage is faster, systems use cache memories to bridge the performance gap between fast processors and slow main memory.

---
### 1. Data Movement in a System
- Example: In the `hello` program, data and instructions move repeatedly:
	1. From disk to main memory when loaded.
	2. From main memory to CPU as instructions execute.
	3. From main memory to display device for output.
- From a programmer's view, these copy operations are **overhead** that slow the "real work" of the program.
- System designers aim to main data transfers as fast as possible.

---
### 2. The Processor-Memory Performance Gap
- **Larger storage devices** (e.g., disk, memory) are **slower**.
- **Smaller storage devices** (e.g., registers) are **faster but more expensive**.

| Storage Type       | Size (approx.)    | Relative Speed                  |
| ------------------ | ----------------- | ------------------------------- |
| Disk               | Terabytes         | ~10,000,000x slower than memory |
| Main Memory (DRAM) | Gigabytes         | ~100x slower than register file |
| Register File      | Hundreds of bytes | Fastest accessible storage      |
- Over time, CPU speed has increased much faster than memory speed, widening the gap.
- This growing gap is a major performance bottleneck in computer systems.

---
### 3. Cache Memories
- **Caches**: Small, fast storage devices between the CPU and main memory.
- Act as temporary staging areas for data and instructions the CPU is likely to reuse soon.
- **Purpose**: Reduce the time needed to access frequently used data.

**Typical Cache Hierarchy**
1. **L1 Cache**:
	- Located on the processor chip.
	- Holds tens of kilobytes.
	- Nearly as fast as the register file.
2. **L2 Cache**:
	- Larger (hundreds of kilobytes to a few megabytes).
	- Connected to CPU via a special bus.
	- About 5x slower than L1, but 5-10x faster than main memory.
3. **L3 Cache** (on newer systems):
	- Even larger, shared among cores.
	- Slower than L2 but still faster than DRAM.
- **Cache Technology**: Implemented with SRAM (Static Random Access Memory), which is faster but more expensive than DRAM.

---
### 4. Principle of Locality
- **Locality**: Programs tend to access **the same data or nearby data** repeatedly within short periods.
- Caches exploit this by storing recently used or nearby instructions/data.
- Result: Systems can behave as if they have both **large memory capacity** and **high speed**.

---
### 5. Implications for Programmers
- Cache-aware programming can improve performance by orders of magnitude.
- Writing code that maximizes locality (e.g., accessing contiguous memory, reusing data efficiently) can greatly reduce memory latency.

---

![[Pasted image 20251111130048.png]]